{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmS6dULYnGoT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import cv2\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import imutils\n",
        "import itertools\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import skimage.draw\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.cm as cm\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.ImageDraw as ImageDraw\n",
        "from imgaug import augmenters as ia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = 'brain-tumor-segmentation/brain_tumor_data/'\n",
        "examples = [Image.open(DATASET_DIR + 'train/100.jpg'),Image.open(DATASET_DIR + 'train/116.jpg'),Image.open(DATASET_DIR+'train/221.jpg')]\n",
        "examplesSeg = ['100.jpg20477','116.jpg10596','221.jpg19584']\n",
        "\n",
        "## print some example images from train directory\n",
        "fig = plt.figure(figsize=(10,14))\n",
        "\n",
        "for i in range(0, len(examples)):\n",
        "    a = fig.add_subplot(1, 3, i+1)\n",
        "    imgplot = plt.imshow(examples[i])\n",
        "    a.set_title('Example '+str(i))\n",
        "\n",
        "## print segment examples for images from json annotations file\n",
        "with open(DATASET_DIR+'train/annotations.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for i in range(0,len(examplesSeg)):\n",
        "        # load regions from json file and transform them into (x,y) coordinates\n",
        "        coord = list(zip(data[examplesSeg[i]]['regions'][0]['shape_attributes']['all_points_x'],data[examplesSeg[i]]['regions'][0]['shape_attributes']['all_points_y']))\n",
        "        image = Image.new(\"RGB\", np.asarray(examples[i]).shape[0:2])\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        draw.polygon((coord), fill=200)\n",
        "        a = fig.add_subplot(2, 3, 3+i+1)\n",
        "        imgplot = plt.imshow(image)\n",
        "        a.set_title('Segment for example ' + str(i))"
      ],
      "metadata": {
        "id": "xUFUGwphnbDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showDataLayout():\n",
        "    values = dict()\n",
        "    for file in os.listdir(DATASET_DIR):\n",
        "        values[str(file)] = len(os.listdir('./brain-tumor-segmentation/brain_tumor_data/' + file))\n",
        "    plt.title('Number of images per folder')\n",
        "    plt.bar(range(len(values)), list(values.values()), align='center',color=[ 'green','red', 'blue'])\n",
        "    plt.xticks(range(len(values)), list(values.keys()))\n",
        "    plt.show()\n",
        "showDataLayout()"
      ],
      "metadata": {
        "id": "f3TELyF3ndGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"mrcnn\")\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "%matplotlib inline\n",
        "# import modify_ann\n",
        "# Import Mask RCNN\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "from PIL import Image, ImageDraw\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"./\")\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "pDuBjaaXne-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.bool=np.bool_ ##important !"
      ],
      "metadata": {
        "id": "J0DocWeynguv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function helps displaying images\n",
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "\n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size * cols, size * rows))\n",
        "    return ax\n"
      ],
      "metadata": {
        "id": "ZekBX0MQnjpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainScanDataset(utils.Dataset):\n",
        "\n",
        "    def load_brain_scan(self, dataset_dir, subset):\n",
        "        \"\"\"Load a subset of the FarmCow dataset.\n",
        "        dataset_dir: Root directory of the dataset.\n",
        "        subset: Subset to load: train or val\n",
        "        \"\"\"\n",
        "        # Add classes. We have only one class to add.\n",
        "        self.add_class(\"tumor\", 1, \"tumor\")\n",
        "\n",
        "        # Train or validation dataset?\n",
        "        assert subset in [\"train\", \"val\", 'test']\n",
        "        dataset_dir = os.path.join(dataset_dir, subset)\n",
        "\n",
        "        annotations = json.load(open(os.path.join(DATASET_DIR, subset, 'annotations'+'.json')))\n",
        "        annotations = list(annotations.values())  # don't need the dict keys\n",
        "\n",
        "        # The VIA tool saves images in the JSON even if they don't have any\n",
        "        # annotations. Skip unannotated images.\n",
        "        annotations = [a for a in annotations if a['regions']]\n",
        "\n",
        "        # Add images\n",
        "        for a in annotations:\n",
        "            # Get the x, y coordinaets of points of the polygons that make up\n",
        "            # the outline of each object instance. These are stores in the\n",
        "            # shape_attributes (see json format above)\n",
        "            # The if condition is needed to support VIA versions 1.x and 2.x.\n",
        "            if type(a['regions']) is dict:\n",
        "                polygons = [r['shape_attributes'] for r in a['regions'].values()]\n",
        "            else:\n",
        "                polygons = [r['shape_attributes'] for r in a['regions']]\n",
        "\n",
        "            # load_mask() needs the image size to convert polygons to masks.\n",
        "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
        "            # the image. This is only managable since the dataset is tiny.\n",
        "            image_path = os.path.join(dataset_dir, a['filename'])\n",
        "            image = skimage.io.imread(image_path)\n",
        "            height, width = image.shape[:2]\n",
        "\n",
        "            self.add_image(\n",
        "                \"tumor\",\n",
        "                image_id=a['filename'],  # use file name as a unique image id\n",
        "                path=image_path,\n",
        "                width=width,\n",
        "                height=height,\n",
        "                polygons=polygons\n",
        "            )\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for an image.\n",
        "       Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a farm_cow dataset image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"tumor\":\n",
        "            return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        # Convert polygons to a bitmap mask of shape\n",
        "        # [height, width, instance_count]\n",
        "        info = self.image_info[image_id]\n",
        "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
        "                        dtype=np.uint8)\n",
        "        for i, p in enumerate(info[\"polygons\"]):\n",
        "            # Get indexes of pixels inside the polygon and set them to 1\n",
        "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
        "            mask[rr, cc, i] = 1\n",
        "\n",
        "        # Return mask, and array of class IDs of each instance. Since we have\n",
        "        # one class ID only, we return an array of 1s\n",
        "        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the path of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"tumor\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)"
      ],
      "metadata": {
        "id": "MGte8mt5nlF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset.\n",
        "dataset_train = BrainScanDataset()\n",
        "dataset_train.load_brain_scan(DATASET_DIR, 'train')\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = BrainScanDataset()\n",
        "dataset_val.load_brain_scan(DATASET_DIR, 'val')\n",
        "dataset_val.prepare()\n",
        "\n",
        "# Test dataset\n",
        "dataset_test = BrainScanDataset()\n",
        "dataset_test.load_brain_scan(DATASET_DIR, 'test')\n",
        "dataset_test.prepare()"
      ],
      "metadata": {
        "id": "gp1d_gxtntA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and display 5 random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids,5)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names,1)"
      ],
      "metadata": {
        "id": "MQziUYTDoEx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model = modellib.MaskRCNN(\n",
        "    mode='training',\n",
        "    config=config,\n",
        "    model_dir=DEFAULT_LOGS_DIR\n",
        ")\n",
        "\n",
        "# Load weights\n",
        "model.load_weights(\n",
        "    'brain_tumor1.h5',\n",
        "    by_name=True,\n",
        ")"
      ],
      "metadata": {
        "id": "a3a7taW-oHzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we're using a very small dataset, and starting from\n",
        "# COCO trained weights, we don't need to train too long. Also,\n",
        "# no need to train all layers, just the heads should do it.\n",
        "print(\"Training network heads\")\n",
        "model.train(\n",
        "    dataset_train, dataset_val,\n",
        "    learning_rate=config.LEARNING_RATE,\n",
        "    epochs=30,\n",
        "    layers='heads'\n",
        ")"
      ],
      "metadata": {
        "id": "DgK11On1oJzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving weights after training\n",
        "model.keras_model.save_weights('brain_tumor.h5')"
      ],
      "metadata": {
        "id": "j82xPEVDoLkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceConfig(BrainTumorConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\",\n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "model_path = \"brain_tumor1.h5\"\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)\n",
        "\n",
        "def get_ax(rows=1, cols=1, size=12):\n",
        "\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "\n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return axi) root@mane:/home/baki# cd Desktop/\n",
        "(taghi) root@mane:/home/baki/Desktop# cd project/image_segmentation/image_segmentation/\n",
        "(taghi) root@mane:/ho\n",
        "\n",
        "image_id=random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config,\n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,\n",
        "                            dataset_train.class_names, figsize=(12, 12))"
      ],
      "metadata": {
        "id": "uTFA8f27oNex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_id=random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config,\n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,\n",
        "                            dataset_train.class_names, figsize=(12, 12))"
      ],
      "metadata": {
        "id": "tr3azkjaoQ4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'],\n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "metadata": {
        "id": "NvZ-1gk1oUH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_plot_differences(dataset, img_id):\n",
        "    original_image, image_meta, gt_class_id, gt_box, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset, config,\n",
        "                               img_id, use_mini_mask=False)\n",
        "\n",
        "    results = model.detect([original_image], verbose=0)\n",
        "    r = results[0]\n",
        "    if len(r['rois']):\n",
        "        visualize.display_differences(\n",
        "            original_image,\n",
        "            gt_box, gt_class_id, gt_mask,\n",
        "            r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
        "            class_names = ['tumor'], title=\"\", ax=get_ax(),\n",
        "            show_mask=True, show_box=True)"
      ],
      "metadata": {
        "id": "PpdEs5UJoXm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We display the original image without annotations first to visualize the differences\n",
        "def display_image(dataset, ind):\n",
        "    plt.figure(figsize=(12,12))\n",
        "    plt.imshow(dataset.load_image(ind))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title('Original Image')\n",
        "    plt.show()\n",
        "display_image(dataset_test, 21)\n",
        "predict_and_plot_differences(dataset_test, 21)"
      ],
      "metadata": {
        "id": "IAdPxfeboX-z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}